{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Explorations in Building Search Functionality for Local Chron Am",
   "id": "ebfba72574609b9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import tarfile\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import ast"
   ],
   "id": "fc06e5799b3e0f70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Method 1: Search by Selected Year Range, State, and Keyword",
   "id": "ea0583a7f9315fb9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/MatthewKollmer/chron_am_backup/refs/heads/main/newspapers.csv')\n",
    "\n",
    "# be sure to change these filepaths accordingly\n",
    "directory = '/Volumes/t5_evo_8tb/ChronAm/tarbiz2_files'\n",
    "search_results = '/Volumes/t5_evo_8tb/ChronAm/search_results'\n",
    "os.makedirs(search_results, exist_ok=True)"
   ],
   "id": "7f7111b34a85cf61",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# change these variables to the state you want to search, the year range (start_year to end_year), and the keyword search\n",
    "# you can search by multiple keywords. Just enter them as a list (i.e. 'search_terms = ['lynching', 'outrage', 'etc', 'etc']\n",
    "\n",
    "state = 'Alaska' # must be full-text spelling since that's how they appear in newspapers.csv, column 'State'\n",
    "start_year = 1903\n",
    "end_year = 1904\n",
    "search_terms = ['lynching']"
   ],
   "id": "4466ea5186849646",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# some necessary functions\n",
    "\n",
    "# this one puts together a list of years within the selected range\n",
    "def overlapping_years(year_list):\n",
    "    return any(start_year <= y <= end_year for y in year_list)\n",
    "\n",
    "# this one takes the search term and compiles all possible OCR variations with one character difference from the standard spelling\n",
    "# it ignores search terms that are four characters or less, however\n",
    "def ocr_variations(term: str) -> str:\n",
    "    words, variations = term.split(), []\n",
    "    for word in words:\n",
    "        if len(word) < 4:\n",
    "            variations.append(re.escape(word))\n",
    "            continue\n",
    "            \n",
    "        potential_ocr_errors = [re.escape(word)]\n",
    "        for i in range(len(word)):\n",
    "            potential_ocr_errors.append(re.escape(word[:i]) + '.' + re.escape(word[i+1:]))\n",
    "        variations.append(f'(?:{\"|\".join(potential_ocr_errors)})')\n",
    "        \n",
    "    return r'\\W+'.join(variations)"
   ],
   "id": "3d82119aefa1c4ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# before running further, these filters ensure your state selection and year range appear somewhere in the data\n",
    "state_selection_filter = df[df['State'].str.strip().str.lower() == state.lower()]\n",
    "if state_selection_filter.empty:\n",
    "    raise SystemExit(f'your state selection does not appear in the newspaper data. Check spelling or results for: {state}')\n",
    "\n",
    "target_tarfiles = set()\n",
    "for _, row in state_selection_filter.iterrows():\n",
    "    for entry in ast.literal_eval(row['tarfiles']):\n",
    "        if isinstance(entry, dict) and overlapping_years(entry['years']):\n",
    "            target_tarfiles.add(entry['file_name'])\n",
    "\n",
    "if not target_tarfiles:\n",
    "    raise SystemExit(f'there does not seem to be any files containing digitized pages from the state ({state}) you selected during the time period you selected ({start_year} - {end_year}).')\n",
    "\n",
    "print(f'Number of files to search through: {len(target_tarfiles)}')"
   ],
   "id": "aeeafad6f5b4b032",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# first, use ocr_variations() to compile possible variations of your search term\n",
    "fuzzy_search_term = [ocr_variations(search_word) for search_word in search_terms]\n",
    "keyword_regex = re.compile('|'.join(fuzzy_search_term), re.IGNORECASE)\n",
    "\n",
    "# then run the search. Heads up: could take a while, especially if you're searching a large timeframe!\n",
    "hit_count = 0\n",
    "for file in tqdm(sorted(target_tarfiles), desc='Searching'):\n",
    "    tarfile_path = os.path.join(directory, file)\n",
    "\n",
    "    try:\n",
    "        with tarfile.open(tarfile_path, 'r:bz2') as tar:\n",
    "            for page in tar:\n",
    "                if not page.isfile() or not page.name.endswith('.txt'):\n",
    "                    continue\n",
    "\n",
    "                year_match = re.search(r'/(\\d{4})/', page.name)\n",
    "                if year_match and not (start_year <= int(year_match.group(1)) <= end_year):\n",
    "                    continue\n",
    "\n",
    "                extracted = tar.extractfile(page)\n",
    "                if extracted is None:\n",
    "                    continue\n",
    "                page_bytes = extracted.read()\n",
    "                page_text  = page_bytes.decode('utf-8', 'ignore')\n",
    "\n",
    "                if not keyword_regex.search(page_text):\n",
    "                    continue\n",
    "\n",
    "                save_name = f'{file}-{page.name.replace(\"/\", \"-\")}'\n",
    "                save_path = os.path.join(search_results, save_name)\n",
    "\n",
    "                with open(save_path, 'wb') as out_fh:\n",
    "                    out_fh.write(page_bytes)\n",
    "\n",
    "                hit_count += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Error processing', file, ':', e)\n",
    "\n",
    "print()\n",
    "print(f'Done! {hit_count} pages were found to contain relevant results.')"
   ],
   "id": "edbbf458978f61b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Method 2: Search by Selected State, Newspaper, and Year Range\n",
    "\n",
    "### And Attempting to Incorporate Widgets as Well"
   ],
   "id": "826e574ee15198c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import shutil\n",
    "import tarfile\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import ast\n",
    "import itertools\n",
    "import ipywidgets as w\n",
    "from IPython.display import display, HTML"
   ],
   "id": "1e3c861ede334cab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "batches_df = pd.read_csv('https://raw.githubusercontent.com/MatthewKollmer/chron_am_backup/refs/heads/main/ocr_batches.csv', converters={'contents': ast.literal_eval}) # converters here ensures the lists of dictionaries in ocr_batches.csv are read as such\n",
    "papers_df = pd.read_csv('https://raw.githubusercontent.com/MatthewKollmer/chron_am_backup/refs/heads/main/newspapers.csv')\n",
    "\n",
    "# be sure to change these directories accordingly\n",
    "directory = '/Volumes/t5_evo_8tb/ChronAm/tarbiz2_files'\n",
    "search_results  = '/Volumes/t5_evo_8tb/ChronAm/state_year_results'\n",
    "os.makedirs(search_results, exist_ok=True)"
   ],
   "id": "d7fbded408a505e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# we'll need a map of sn codes and the tarfiles containing them\n",
    "sn_to_tarfiles = {}\n",
    "for _, batch_row in batches_df.iterrows():\n",
    "    tarball = batch_row['file_name']\n",
    "    for item in batch_row['contents']:\n",
    "        sn_code, info = next(iter(item.items()))\n",
    "        sn_to_tarfiles.setdefault(sn_code, []).append({'file_name': tarball, 'years': info['years']})"
   ],
   "id": "cf1f8cc3090ffa7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# assembling the widgets\n",
    "state_selection_dropdown = w.Dropdown(options=sorted(papers_df['State'].dropna().unique()), description='State:')\n",
    "\n",
    "min_year = min(itertools.chain.from_iterable(dictionary['years'] for list in sn_to_tarfiles.values() for dictionary in list))\n",
    "max_year = max(itertools.chain.from_iterable(dictionary['years'] for list in sn_to_tarfiles.values() for dictionary in list))\n",
    "\n",
    "year_slider = w.IntRangeSlider(value=[min_year, max_year], min=min_year, max=max_year, step=1, description='Search Year Range:', continuous_update=False, layout=w.Layout(width='70%'))\n",
    "\n",
    "paper_select = w.SelectMultiple(options=[], description='Newspapers:', layout=w.Layout(width='95%', height='200px'))\n",
    "search_button = w.Button(description='Start Search')\n",
    "out_box = w.Output()"
   ],
   "id": "ccc2ad8c9b981ca3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# some necessary functions\n",
    "\n",
    "# a function for assembling the full range of selectable years in the database\n",
    "def full_year_range(year_list, year0, year1):\n",
    "    return any(year0 <= year <= year1 for year in year_list)\n",
    "\n",
    "# a function for assembling the full list of newspapers in the database\n",
    "def update_newspaper_list(*args):\n",
    "    state = state_selection_dropdown.value\n",
    "    year0, year1 = year_slider.value\n",
    "    subset = papers_df[papers_df['State'] == state]\n",
    "\n",
    "    options = []\n",
    "    for _, row in subset.iterrows():\n",
    "        sn_code = row['LCCN']\n",
    "        title = row['Title']\n",
    "        if any(full_year_range(entry['years'], year0, year1) for entry in sn_to_tarfiles.get(sn_code, [])):\n",
    "            options.append(f'{sn_code} — {title}')\n",
    "            \n",
    "    paper_select.options = sorted(options)\n",
    "    paper_select.value = ()\n",
    "\n",
    "# function to run the search via the widgets created above\n",
    "def run_search(_):\n",
    "    out_box.clear_output()\n",
    "    chosen = paper_select.value\n",
    "    if not chosen:\n",
    "        with out_box: print('Select a newspaper')\n",
    "        return\n",
    "\n",
    "    year0, year1 = year_slider.value\n",
    "    saved_results = os.path.join(search_results, f'{state_selection_dropdown.value}_{year0}-{year1}')\n",
    "    os.makedirs(saved_results, exist_ok=True)\n",
    "\n",
    "    tarfiles_needed = set()\n",
    "    chosen_sn_code       = []\n",
    "    for label in chosen:\n",
    "        sn_code = label.split(' — ')[0]\n",
    "        chosen_sn_code.append(sn_code)\n",
    "        for entry in sn_to_tarfiles[sn_code]:\n",
    "            if full_year_range(entry['years'], year0, year1):\n",
    "                tarfiles_needed.add(entry['file_name'])\n",
    "\n",
    "    with out_box:\n",
    "        print()\n",
    "        print(f'Searching {len(tarfiles_needed)} batches ...')\n",
    "        print()\n",
    "\n",
    "    year_regex = re.compile(r'/(\\d{4})/')\n",
    "    with out_box:\n",
    "        for file in tqdm(sorted(tarfiles_needed), desc='Searching'):\n",
    "            tarfile_path = os.path.join(directory, file)\n",
    "            try:\n",
    "                with tarfile.open(tarfile_path, 'r:bz2') as tar:\n",
    "                    for page in tar:\n",
    "                        if not page.isfile() or not page.name.endswith('.txt'):\n",
    "                            continue\n",
    "\n",
    "                        if not any(sn_code in page.name for sn_code in chosen_sn_code):\n",
    "                            continue\n",
    "\n",
    "                        year_match = year_regex.search(page.name)\n",
    "                        if not year_match:\n",
    "                            continue\n",
    "                        year = int(year_match.group(1))\n",
    "                        if not (year0 <= year <= year1):\n",
    "                            continue\n",
    "\n",
    "                        save_name = f'{file}-{page.name.replace(\"/\", \"-\")}'\n",
    "                        save_path = os.path.join(saved_results, save_name)\n",
    "                        with open(save_path, 'wb') as fh_out:\n",
    "                            shutil.copyfileobj(tar.extractfile(page), fh_out)\n",
    "            except Exception as e:\n",
    "                with out_box: print('Error processing️', file, e)\n",
    "\n",
    "    with out_box:\n",
    "        print()\n",
    "        print('Search complete.')\n",
    "        display(HTML(f'<b>Files saved in:</b> {saved_results}'))"
   ],
   "id": "5686f80c91ab4141",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# if you're ready to pull pages by state, year range, and newspaper, run this code\n",
    "state_selection_dropdown.observe(update_newspaper_list, names='value')\n",
    "year_slider.observe(update_newspaper_list, names='value')\n",
    "update_newspaper_list()\n",
    "\n",
    "search_button.on_click(run_search)\n",
    "display(state_selection_dropdown, year_slider, paper_select, search_button, out_box)"
   ],
   "id": "a30d27f188542f59",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "85cec337379d27f5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
