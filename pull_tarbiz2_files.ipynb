{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Scraping ChronAm OCR Files (from tarbiz2 batches)",
   "id": "fbebab97637ccd84"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm.notebook import tqdm # This best if you're working in a Jupyter notebook. Otherwise, try 'from tqdm import tqdm'\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1) Overview\n",
    "\n",
    "The following notebook presents code for scraping the OCR-generated text files of Chronicling America. It's a step toward backing up Chronicling America and using the database locally. To replicate this process, you'll need at least 3TB of space. You will also need a solid Internet connection for 3+ days.\n",
    "\n",
    "For more info about this project, [see my blog post](https://matthewkollmer.com/how-im-backing-up-chronicling-america/) describing the instigation and larger process."
   ],
   "id": "2586f85f6b67c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2) Pulling Tarbiz2 Files\n",
    "\n",
    "All of the Chronicling America text data is stored in tarball files here: [https://chroniclingamerica.loc.gov/data/ocr/](https://chroniclingamerica.loc.gov/data/ocr/). Using this page and the [ocr_batches.csv file I created](https://github.com/MatthewKollmer/chron_am_backup/blob/main/ocr_batches.csv), the code below which pulls each tarball and saves it to your chosen directory. This is one step toward backing up Chronicling America and being able to use it locally."
   ],
   "id": "d8d6f54fa04c727a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ocr_batches = pd.read_csv('ocr_batches.csv')\n",
    "base_url = 'https://chroniclingamerica.loc.gov/data/ocr/'\n",
    "# be sure to set your the output directory to your own external hard drive/SSD with at least 3TB of space\n",
    "output_directory = 'CHANGE/TO/DIRECTORY/PATH/OF/YOUR/CHOOSING'"
   ],
   "id": "2bd607dadc4c3100",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Heads up: this code runs for a long, long time. On my device, it took about 80 hours. It's okay to stop and start it againâ€“the first conditional statement in the pull_tarbiz_files() function (where it reads: \"if os.path.exists(output_path):...\") skips over files that have already been downloaded, so it's fine if you want to run the code for a while, stop running it, and then start again later. You'll just pick up where you left off.\n",
    "progress_bar = tqdm(total=len(ocr_batches), unit='file', desc='Batches Downloaded', mininterval=1.0)\n",
    "\n",
    "def pull_tarbiz_files(file_name):\n",
    "    url = f'{base_url}{file_name}'\n",
    "    output_path = os.path.join(output_directory, file_name)\n",
    "\n",
    "    if os.path.exists(output_path):\n",
    "        progress_bar.update(1)\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with requests.get(url, stream=True) as response:\n",
    "            if response.status_code == 200:\n",
    "                with open(output_path, 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "                    \n",
    "                progress_bar.update(1)\n",
    "                time.sleep(60) # This adds a minute in between each download to respect Chron Am's rate limits. You could try to shorten this timespan to speed up the whole process, but I was still getting 429 errors with time.sleep(30), so I just bumped it up to a minute, which kept things running. \n",
    "\n",
    "            elif response.status_code == 429:\n",
    "                print('Received 429 error. Sorry Chron Am! Waiting 1 hour before retrying.')\n",
    "                time.sleep(3600) # Idk if Chron Am bans IP addresses, but just in case, better back off for an hour if you somehow get a 429 error!\n",
    "\n",
    "            else:\n",
    "                print(f'Sumpin went wrong downloading {file_name}: {response.status_code}')\n",
    "                time.sleep(5)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Exception issue with {file_name}: {e}')\n",
    "        time.sleep(5)\n",
    "\n",
    "for _, row in ocr_batches.iterrows():\n",
    "    pull_tarbiz_files(row['file_name'])\n",
    "\n",
    "progress_bar.close()"
   ],
   "id": "6b4e0818f1c9e351",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
