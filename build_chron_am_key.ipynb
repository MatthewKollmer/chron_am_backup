{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Building Chronicling America Search Keys",
   "id": "5bd9a07d86bf0f4a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1) Overview\n",
    "\n",
    "The following notebook shows how I compiled the metadata for Chronicling America's newspapers and batch files. These metadata can be viewed here:\n",
    "\n",
    "- OCR Batch file data: [https://github.com/MatthewKollmer/chron_am_backup/blob/main/ocr_batches.csv](https://github.com/MatthewKollmer/chron_am_backup/blob/main/ocr_batches.csv)\n",
    "- Chronicling America newspaper data: [https://github.com/MatthewKollmer/chron_am_backup/blob/main/newspapers.csv](https://github.com/MatthewKollmer/chron_am_backup/blob/main/newspapers.csv)\n",
    "\n",
    "In further steps, I plan to use these dataframes to make Chron Am searchable on my local device."
   ],
   "id": "2b5f4993121bbbe3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2) Pulling OCR Batch Metadata\n",
    "\n",
    "Before working in Python, I began by copy-and-pasting the contents of [this Chron Am batch directory](https://chroniclingamerica.loc.gov/data/ocr/) into BBEdit. Then I used regular expressions to separate the contents with commas rather than spaces, creating a csv file called 'ocr_batches.csv' (a critical step for my [pull_tarbiz2_files.ipynb notebook](https://github.com/MatthewKollmer/chron_am_backup/blob/main/pull_tarbiz2_files.ipynb)). This gave me a reference to make calls to download the tarball files containing Chron Am's OCR.\n",
    "\n",
    "But I also needed to keep track of what newspapers and years were contained in each tarball, so I also scraped Chron Am's batch description pages. The following code demonstrates how I did it. Basically, it calls the associated batch html page and pulls its table data for each newspaper and year range, saving those details in dictionaries for each newspaper in the file."
   ],
   "id": "c9b73e48a81f1ea8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "batches = pd.read_csv('ocr_batches.csv') # online at https://raw.githubusercontent.com/MatthewKollmer/chron_am_backup/refs/heads/main/ocr_batches.csv \n",
    "batches['batch_url'] = 'https://chroniclingamerica.loc.gov/batches/' + batches['file_name'].str.replace('.tar.bz2', '', regex=False) + '/'\n",
    "batches['contents'] = None\n",
    "batches.head()"
   ],
   "id": "f4837f52e26beaa3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "year_range_regex = re.compile(r'(\\d{4}) -.*(\\d{4})')\n",
    "sn_code_regex = re.compile(r'\\((sn?\\d+|\\d+)\\)')\n",
    "\n",
    "def pull_batch_metadata(batch_url: str) -> list[dict]:\n",
    "    metadata_entries: list[dict] = []\n",
    "    with requests.get(batch_url, timeout=30) as response:\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            header_tag = soup.find('h4', string='Reels in Batch')\n",
    "            data_table = header_tag.find_next('table')\n",
    "            \n",
    "            for row in data_table.find_all('tr')[1:]:\n",
    "                table_cells = row.find_all('td')\n",
    "                \n",
    "                date_text = table_cells[2].get_text(' ', strip=True)\n",
    "                year_match = year_range_regex.search(date_text)\n",
    "                start_year = int(year_match.group(1))\n",
    "                end_year = int(year_match.group(2))\n",
    "                full_years = list(range(start_year, end_year + 1))\n",
    "    \n",
    "                for title in table_cells[1].find_all('a'):\n",
    "                    title_text = title.get_text(' ', strip=True)\n",
    "                    sn_match = sn_code_regex.search(title_text)\n",
    "                    sn_code = sn_match.group(1)\n",
    "                    title = title_text.replace(f'({sn_code})', '').strip()\n",
    "                    metadata_entries.append({sn_code: {'newspaper_title': title, 'years': full_years}})\n",
    "                \n",
    "            time.sleep(2) # this seems to work when calling these html pages. If you get 429 errors, add to the delay.\n",
    "\n",
    "        elif response.status_code == 429:\n",
    "            print('Received 429 error. Sorry Chron Am! Waiting 1 hour before retrying.')\n",
    "            time.sleep(3600)\n",
    "\n",
    "        else:\n",
    "            print(f'Sumpin went wrong at {batch_url}: {response.status_code}')\n",
    "            time.sleep(5)\n",
    "\n",
    "    return metadata_entries\n",
    "\n",
    "progress_bar = tqdm(total=len(batches), desc='Metadata scraped', unit='batch', mininterval=1.0)\n",
    "\n",
    "for row, batch_row in batches.iterrows():\n",
    "    metadata = pull_batch_metadata(batch_row['batch_url'])\n",
    "    batches.at[row, 'contents'] = metadata\n",
    "    progress_bar.update(1)\n",
    "    \n",
    "batches"
   ],
   "id": "b02795405d5966d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3) Merging sn_code Dictionaries\n",
    "\n",
    "Some newspapers were separated into different parts of the batch file (groupings that Chron Am calls 'reels'). To account for this, I merged dictionaries by sn_code, thereby having full year ranges per newspaper present in each tarball."
   ],
   "id": "29aed3a3912452dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def merge_dictionaries_by_sn_code(contents):\n",
    "    dictionary_key: dict[str, dict] = {}\n",
    "    for single_dictionary in contents:\n",
    "        sn_code, info = next(iter(single_dictionary.items()))\n",
    "        if sn_code in dictionary_key:\n",
    "            dictionary_key[sn_code]['years'].extend(info['years'])\n",
    "\n",
    "        else:\n",
    "            dictionary_key[sn_code] = {'newspaper_title': info['newspaper_title'], 'years': info['years'].copy()}\n",
    "\n",
    "    for sn_code in dictionary_key:\n",
    "        dictionary_key[sn_code]['years'] = sorted(set(dictionary_key[sn_code]['years']))\n",
    "\n",
    "    return [{sn: data} for sn, data in dictionary_key.items()]\n",
    "\n",
    "batches['contents'] = batches['contents'].apply(merge_dictionaries_by_sn_code)\n",
    "\n",
    "batches"
   ],
   "id": "c496cb43034107bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "batches.to_csv('ocr_batches.csv', index=False)",
   "id": "fbf205a4b8361b4e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4) Enriching newspapers.csv\n",
    "\n",
    "I also thought it would be good to have a version of the data where each row represents a newspaper/sn_code rather than a tarball file. I started with [Chron Am's own file of newspapers](https://chroniclingamerica.loc.gov/newspapers.txt), downloading it and then converting it to a csv file in BBEdit. Then I cross-referenced this file's 'LCCN' column with the dictionaries in 'ocr_batches.csv' so it would have which tarballs contained which newspapers."
   ],
   "id": "21b496c22940bd0b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "newspapers = pd.read_csv('newspapers.csv') # online at https://raw.githubusercontent.com/MatthewKollmer/chron_am_backup/refs/heads/main/newspapers.csv\n",
    "\n",
    "sn_to_tarfiles = {}\n",
    "\n",
    "for row, batch_row in batches.iterrows():\n",
    "    tarball_name = batch_row['file_name']\n",
    "    for entry in batch_row['contents']:\n",
    "        sn_code, info = next(iter(entry.items()))\n",
    "        sn_to_tarfiles.setdefault(sn_code, []).append({'file_name': tarball_name, 'years': info['years']})\n",
    "\n",
    "newspapers['tarfiles'] = newspapers['LCCN'].map(lambda code: sn_to_tarfiles.get(code, []))\n",
    "\n",
    "newspapers"
   ],
   "id": "385c1eecc51b68cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "newspapers.to_csv('newspapers.csv', index=False)",
   "id": "840dca0792d4020a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
